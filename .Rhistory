res = evaluate(x = eval_sets,
method = "UBCF", parameter = list(method = "Jaccard"),
n = seq(10,100,10))
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
res = evaluate(x = eval_sets,
method = "UBCF", parameter = list(method = "Jaccard"),
n = seq(10,100,10))
#Se usa validación cruzada para validar las recomendaciones.
eval_sets = evaluationScheme(data = bf.bin,
method = "cross-validation",
k = 5,
given = 15)
set.seed(2019)
bf.bin = bf.bin[rowCounts(bf.bin) > 40,
colCounts(bf.bin) > 80]
#Se usa validación cruzada para validar las recomendaciones.
eval_sets = evaluationScheme(data = bf.bin,
method = "cross-validation",
k = 5,
given = 15)
modelos = list(IBCF_cos = list(name = "IBCF", params = list(method = "cosine")),
IBCF_cor = list(name = "IBCF", params = list(method = "pearson")),
IBCF_jac = list(name = "IBCF", params = list(method = "Jaccard")),
UBCF_cos = list(name = "UBCF", params = list(method = "cosine")),
UBCF_cor = list(name = "UBCF", params = list(method = "pearson")),
UBCF_jac = list(name = "UBCF", params = list(method = "Jaccard")),
random = list(name = "RANDOM", params = NULL))
resul = evaluate(x = eval_sets,
method = modelos,
n = c(1:7, seq(10,100,10)))
plot(resul, annotate = 1, legend = "topleft") + title("Curva AUC")
plot(resul, "prec/rec", annotate = 1, legend = "bottomright") + title("Precisión vs Eficacia")
res = evaluate(x = eval_sets,
method = "UBCF", parameter = list(method = "Jaccard"),
n = seq(10,100,10))
getConfusionMatrix(res)[[1]]
plot(res, "prec/rec", annotate = T, main = "preciosión vs Eficacia")
?Recommender
knitr::opts_chunk$set(echo = TRUE)
#Se muestran 12 recomendaciones para los 5 primeros usuarios, de esta manera comprobamos que funcione correctamente.
rec.matrix[,1:5]
#Se procede a la predicción con el subconjunto de test.
pred = predict(object = recom_hibrida, newdata = test, n = 12)
set.seed(2019)
#Seleccion de usuarios con al menos 40 compras y productos comprados como mínimo 80 veces.
bf.bin = bf.bin[rowCounts(bf.bin) > 40,
colCounts(bf.bin) > 80]
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
bf.df = read_csv("BlackFriday.csv") #Se cargan directamente los datos en una tibble
bf.df %>% summary() #Se comprueban los datos que contiene para saber como tratarlos
bf.df %>% str()
bf.df %>% head()
bf.df %>% tail()
bf.df = mutate(bf.df[,-c(10:11)]) %>% na.omit %>% droplevels() #Se eliminan NAs (Especialmente concentrados en las columnas Product_Category 2 y 3)
bf.df = bf.df[!duplicated(bf.df),] #Se eliminan duplicados en caso de haberlos
bf.df$User_ID = bf.df$User_ID - 1000000 #Se convierten User_ID en números del 1 al 6040
bf.df$User_ID = as.factor(bf.df$User_ID) #Se transforman a factor los datos que se considera oportuno
bf.df$Product_ID = as.factor(bf.df$Product_ID)
bf.df$Gender = as.factor(bf.df$Gender)
bf.df$Age = as.factor(bf.df$Age)
bf.df$City_Category = as.factor(bf.df$City_Category)
bf.df$Marital_Status = as.factor(bf.df$Marital_Status)
bf.df$Stay_In_Current_City_Years = as.factor(bf.df$Stay_In_Current_City_Years)
bf.df$Product_Category_1 = as.factor(bf.df$Product_Category_1)
bf.df$Occupation = as.factor(bf.df$Occupation)
bf.df %>% summary() #De nuevo se comprueban los datos para confirmar que ya estén "limpios"
bf.df %>% str()
#En mi pc tarda un poco en cargar los gráficos. Espero que con un procesador más potente sea más fluido.
#knitr::include_app("http://127.0.0.1:4231", height = 750)
bf.df %>%  ggplot(aes(x = Product_Category_1, y = Purchase, color = Product_Category_1)) + geom_boxplot(show.legend = F) + xlab("Categoría de producto") + ylab("Gasto") + ggtitle("Gasto por categoría de producto") + theme(plot.title = element_text(hjust = 0.5))
bf.df %>% ggplot(aes(x = Product_Category_1, y = Purchase, color = Age)) + geom_bar(show.legend = T, stat = "identity") + xlab("Categoría de producto") + ylab("Gasto") + ggtitle("Gasto por categoría de producto mujeres frente a hombres") + facet_wrap(~ Gender) + theme(plot.title = element_text(hjust = 0.5))
bfcant.df = bf.df %>% group_by(User_ID) %>% summarise(Gender = Gender[1],Age = Age[1],Occupation = Occupation[1], City_Category = City_Category[1], Stay_In_Current_City_Years = Stay_In_Current_City_Years[1], Marital_Status = Marital_Status[1], Item_Count = n())
#Se juntan los compradores para no desvirtuar los resultados de las próximas graficas al cruzar variables factor entre ellas ya que de no ser así se contabilizarían de forma multiple los usuarios con más de una compra.
ggpairs(bfcant.df[,c(5:8)], cardinality_threshold = 21, aes(color = bfcant.df$Gender, alpha = 0.5))
#Se transforma el data frame pasando cada factor de la columna Product_ID a una variable propia con el ID del producto y de esta manera transformar el data frame en una matriz binaria con 0 en caso de no haber comprado el producto y 1 en caso de si haberlo comprado.
bfcant.df = bf.df %>%
gather(observation, Val, Product_ID) %>%
group_by(User_ID,observation, Val) %>%
summarise(n= n()) %>%
ungroup() %>%
spread(Val, n, fill=0)
#Se eliminan las columnas 1 y 2 que corresponden a User_ID (ya recogido en el índice) y a una columna que recoge "Product_ID" ya que es el nombre de la columna original.
bfcant.df = bfcant.df[,-c(1,2)]
#Por ultimo se transforma el data frame en una binaryRatingMatrix para poder trabajar y entrenar el modelo.
bf.bin = data.matrix(bfcant.df)
bf.bin = as(bf.bin, "binaryRatingMatrix")
set.seed(2019)
#Seleccion de usuarios con al menos 40 compras y productos comprados como mínimo 80 veces.
bf.bin = bf.bin[rowCounts(bf.bin) > 40,
colCounts(bf.bin) > 80]
#Se usa validación cruzada para validar las recomendaciones.
eval_sets = evaluationScheme(data = bf.bin,
method = "cross-validation",
k = 5,
given = 20)
#Los modelos a comparar son basados en usuarios, basados en items (mediante distintos metodos), y por ultimo un modelo aleatorio.
modelos = list(IBCF.cos = list(name = "IBCF", params = list(method = "cosine")),
IBCF.cor = list(name = "IBCF", params = list(method = "pearson")),
IBCF.jac = list(name = "IBCF", params = list(method = "Jaccard")),
UBCF.cos = list(name = "UBCF", params = list(method = "cosine")),
UBCF.cor = list(name = "UBCF", params = list(method = "pearson")),
UBCF.jac = list(name = "UBCF", params = list(method = "Jaccard")),
random = list(name = "RANDOM", params = NULL))
resul = evaluate(x = eval_sets,
method = modelos,
n = c(1:7, seq(10,100,10)))
plot(resul, annotate = 1, legend = "topleft") + title("Curva AUC")
plot(resul, "prec/rec", annotate = 1, legend = "bottomright") + title("Precisión vs Eficacia")
res.jac = evaluate(x = eval_sets,
method = "UBCF", parameter = list(method = "Jaccard"),
n = seq(10,100,10))
getConfusionMatrix(res.jac)[[1]]
plot(res.jac, "prec/rec", annotate = T, main = "Precisión vs Eficacia (UBFC Jaccard)")
#Se separan los datos en dos grupos, uno de entrenamiento y otro de prueba.
sep = sample(x= c(T, F),
size = nrow(bf.bin),
replace = T,
prob = c(0.8, 0.2))
train = bf.bin[sep,]
test = bf.bin[!sep,]
#Se entrena el modelo con el subconjunto de entrenamiento.
recom_hibrida = HybridRecommender(
Recommender(train, method = "UBCF", parameter = list(method = "Jaccard")),
Recommender(train, method = "RANDOM"),
weights = c(0.85, 0.15))
#Se procede a la predicción con el subconjunto de prueba.
pred = predict(object = recom_hibrida, newdata = test, n = 12)
rec.matrix = sapply(pred@items, function(x){
colnames(bf.bin[,x])
})
#Se muestran 12 recomendaciones para los 5 primeros usuarios, de esta manera comprobamos que funcione correctamente.
rec.matrix[,1:5]
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
bf.df = read_csv("BlackFriday.csv") #Se cargan directamente los datos en una tibble
bf.df %>% summary() #Se comprueban los datos que contiene para saber como tratarlos
bf.df %>% str()
bf.df %>% head()
bf.df %>% tail()
bf.df = mutate(bf.df[,-c(10:11)]) %>% na.omit %>% droplevels() #Se eliminan NAs (Especialmente concentrados en las columnas Product_Category 2 y 3)
bf.df = bf.df[!duplicated(bf.df),] #Se eliminan duplicados en caso de haberlos
bf.df$User_ID = bf.df$User_ID - 1000000 #Se convierten User_ID en números del 1 al 6040
bf.df$User_ID = as.factor(bf.df$User_ID) #Se transforman a factor los datos que se considera oportuno
bf.df$Product_ID = as.factor(bf.df$Product_ID)
bf.df$Gender = as.factor(bf.df$Gender)
bf.df$Age = as.factor(bf.df$Age)
bf.df$City_Category = as.factor(bf.df$City_Category)
bf.df$Marital_Status = as.factor(bf.df$Marital_Status)
bf.df$Stay_In_Current_City_Years = as.factor(bf.df$Stay_In_Current_City_Years)
bf.df$Product_Category_1 = as.factor(bf.df$Product_Category_1)
bf.df$Occupation = as.factor(bf.df$Occupation)
bf.df %>% summary() #De nuevo se comprueban los datos para confirmar que ya estén "limpios"
bf.df %>% str()
install.packages("rlang")
install.packages("ggplot2")
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
bf.df = read_csv("BlackFriday.csv") #Se cargan directamente los datos en una tibble
bf.df %>% summary() #Se comprueban los datos que contiene para saber como tratarlos
bf.df %>% str()
bf.df %>% head()
bf.df %>% tail()
bf.df = mutate(bf.df[,-c(10:11)]) %>% na.omit %>% droplevels() #Se eliminan NAs (Especialmente concentrados en las columnas Product_Category 2 y 3)
bf.df = bf.df[!duplicated(bf.df),] #Se eliminan duplicados en caso de haberlos
bf.df$User_ID = bf.df$User_ID - 1000000 #Se convierten User_ID en números del 1 al 6040
bf.df$User_ID = as.factor(bf.df$User_ID) #Se transforman a factor los datos que se considera oportuno
bf.df$Product_ID = as.factor(bf.df$Product_ID)
bf.df$Gender = as.factor(bf.df$Gender)
bf.df$Age = as.factor(bf.df$Age)
bf.df$City_Category = as.factor(bf.df$City_Category)
bf.df$Marital_Status = as.factor(bf.df$Marital_Status)
bf.df$Stay_In_Current_City_Years = as.factor(bf.df$Stay_In_Current_City_Years)
bf.df$Product_Category_1 = as.factor(bf.df$Product_Category_1)
bf.df$Occupation = as.factor(bf.df$Occupation)
bf.df %>% summary() #De nuevo se comprueban los datos para confirmar que ya estén "limpios"
bf.df %>% str()
#En mi pc tarda un poco en cargar los gráficos. Espero que con un procesador más potente sea más fluido.
#knitr::include_app("http://127.0.0.1:4231", height = 750)
bf.df %>%  ggplot(aes(x = Product_Category_1, y = Purchase, color = Product_Category_1)) + geom_boxplot(show.legend = F) + xlab("Categoría de producto") + ylab("Gasto") + ggtitle("Gasto por categoría de producto") + theme(plot.title = element_text(hjust = 0.5))
bf.df %>% ggplot(aes(x = Product_Category_1, y = Purchase, color = Age)) + geom_bar(show.legend = T, stat = "identity") + xlab("Categoría de producto") + ylab("Gasto") + ggtitle("Gasto por categoría de producto mujeres frente a hombres") + facet_wrap(~ Gender) + theme(plot.title = element_text(hjust = 0.5))
bfcant.df = bf.df %>% group_by(User_ID) %>% summarise(Gender = Gender[1],Age = Age[1],Occupation = Occupation[1], City_Category = City_Category[1], Stay_In_Current_City_Years = Stay_In_Current_City_Years[1], Marital_Status = Marital_Status[1], Item_Count = n())
#Se juntan los compradores para no desvirtuar los resultados de las próximas graficas al cruzar variables factor entre ellas ya que de no ser así se contabilizarían de forma multiple los usuarios con más de una compra.
ggpairs(bfcant.df[,c(5:8)], cardinality_threshold = 21, aes(color = bfcant.df$Gender, alpha = 0.5))
#Se transforma el data frame pasando cada factor de la columna Product_ID a una variable propia con el ID del producto y de esta manera transformar el data frame en una matriz binaria con 0 en caso de no haber comprado el producto y 1 en caso de si haberlo comprado.
bfcant.df = bf.df %>%
gather(observation, Val, Product_ID) %>%
group_by(User_ID,observation, Val) %>%
summarise(n= n()) %>%
ungroup() %>%
spread(Val, n, fill=0)
#Se eliminan las columnas 1 y 2 que corresponden a User_ID (ya recogido en el índice) y a una columna que recoge "Product_ID" ya que es el nombre de la columna original.
bfcant.df = bfcant.df[,-c(1,2)]
#Por ultimo se transforma el data frame en una binaryRatingMatrix para poder trabajar y entrenar el modelo.
bf.bin = data.matrix(bfcant.df)
bf.bin = as(bf.bin, "binaryRatingMatrix")
set.seed(2019)
#Seleccion de usuarios con al menos 40 compras y productos comprados como mínimo 80 veces.
bf.bin = bf.bin[rowCounts(bf.bin) > 40,
colCounts(bf.bin) > 80]
#Se usa validación cruzada para validar las recomendaciones.
eval_sets = evaluationScheme(data = bf.bin,
method = "cross-validation",
k = 5,
given = 20)
#Los modelos a comparar son basados en usuarios, basados en items (mediante distintos metodos), y por ultimo un modelo aleatorio.
modelos = list(IBCF.cos = list(name = "IBCF", params = list(method = "cosine")),
IBCF.cor = list(name = "IBCF", params = list(method = "pearson")),
IBCF.jac = list(name = "IBCF", params = list(method = "Jaccard")),
UBCF.cos = list(name = "UBCF", params = list(method = "cosine")),
UBCF.cor = list(name = "UBCF", params = list(method = "pearson")),
UBCF.jac = list(name = "UBCF", params = list(method = "Jaccard")),
random = list(name = "RANDOM", params = NULL))
resul = evaluate(x = eval_sets,
method = modelos,
n = c(1:7, seq(10,100,10)))
plot(resul, annotate = 1, legend = "topleft") + title("Curva AUC")
plot(resul, "prec/rec", annotate = 1, legend = "bottomright") + title("Precisión vs Eficacia")
res.jac = evaluate(x = eval_sets,
method = "UBCF", parameter = list(method = "Jaccard"),
n = seq(10,100,10))
getConfusionMatrix(res.jac)[[1]]
plot(res.jac, "prec/rec", annotate = T, main = "Precisión vs Eficacia (UBFC Jaccard)")
#Se separan los datos en dos grupos, uno de entrenamiento y otro de prueba.
sep = sample(x= c(T, F),
size = nrow(bf.bin),
replace = T,
prob = c(0.8, 0.2))
train = bf.bin[sep,]
test = bf.bin[!sep,]
#Se entrena el modelo con el subconjunto de entrenamiento.
recom_hibrida = HybridRecommender(
Recommender(train, method = "UBCF", parameter = list(method = "Jaccard")),
Recommender(train, method = "RANDOM"),
weights = c(0.85, 0.15))
#Se procede a la predicción con el subconjunto de prueba.
pred = predict(object = recom_hibrida, newdata = test, n = 12)
rec.matrix = sapply(pred@items, function(x){
colnames(bf.bin[,x])
})
#Se muestran 12 recomendaciones para los 5 primeros usuarios, de esta manera comprobamos que funcione correctamente.
rec.matrix[,1:5]
knitr::opts_chunk$set(echo = TRUE)
res.jac
getConfusionMatrix
res.jac
(res.jac)[[1]]
getConfusionMatrix(res.jac)[[1]]
?getConfusionMatrix
install.packages("reticulate")
library(reticulate)
use_python("/Users/Javo/Anaconda3")
use_python("/Users/Javo/Anaconda3/python")
use_python("~/Users/Javo/Anaconda3/python")
use_python("~/Users/Javo/Anaconda3")
use_python("~/Users/Javo/Anaconda3/python.exe")
use_python("/Users/Javo/Anaconda3/python.exe")
use_python("C:\Users\Javo\Anaconda3")
use_python("\Users\Javo\Anaconda3")
use_python("/Users/Javo/Anaconda3")
use_condaenv("myenv")
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
library(reticulate)
library(reticulate)
os = import("os")
os$listdir(".")
import matplotlib.pyplot as plt
import pandas
library(reticulate)
use_python("~/Anaconda3/python.exe", requiered = T)
use_python("~/Anaconda3/python.exe", required = T)
use_python("~\Anaconda3\python.exe", required = T)
use_python("~/Anaconda3/python.exe", required = T)
py_config()
import matplotlib.pyplot as plt
use_condaenv()
use_condaenv("env")
py_config()
knitr::opts_chunk$set(echo = TRUE)
py_config()
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
library(reticulate)
py_config()
use_condaenv("env")
py_config()
bf.df = read_csv("BlackFriday.csv") #Se cargan directamente los datos en una tibble
bf.df %>% summary() #Se comprueban los datos que contiene para saber como tratarlos
bf.df %>% str()
bf.df %>% head()
bf.df %>% tail()
bf.df = mutate(bf.df[,-c(10:11)]) %>% na.omit %>% droplevels() #Se eliminan NAs (Especialmente concentrados en las columnas Product_Category 2 y 3)
bf.df = bf.df[!duplicated(bf.df),] #Se eliminan duplicados en caso de haberlos
bf.df$User_ID = bf.df$User_ID - 1000000 #Se convierten User_ID en números del 1 al 6040
bf.df$User_ID = as.factor(bf.df$User_ID) #Se transforman a factor los datos que se considera oportuno
bf.df$Product_ID = as.factor(bf.df$Product_ID)
bf.df$Gender = as.factor(bf.df$Gender)
bf.df$Age = as.factor(bf.df$Age)
bf.df$City_Category = as.factor(bf.df$City_Category)
bf.df$Marital_Status = as.factor(bf.df$Marital_Status)
bf.df$Stay_In_Current_City_Years = as.factor(bf.df$Stay_In_Current_City_Years)
bf.df$Product_Category_1 = as.factor(bf.df$Product_Category_1)
bf.df$Occupation = as.factor(bf.df$Occupation)
bf.df %>% summary() #De nuevo se comprueban los datos para confirmar que ya estén "limpios"
bf.df %>% str()
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python(C:\Users\Javo\Anaconda3\python.exe)
use_python("C:\Users\Javo\Anaconda3\python.exe")
use_python("/Users/Javo/Anaconda3/python.exe")
py_config()
use_python("~/Users/Javo/Anaconda3/python.exe")
py_config()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
library(reticulate)
py_config()
import pandas
difflib = import("difflib")
np = import("numpy", convert = F)
a = np_array(c(1:8), dtype = "float16")
a
type(a)
class(a)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
library(reticulate)
py_config()
plot(py$c)
plot(py$bf)
knitr::opts_chunk$set(echo = TRUE)
plot(py$c)
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
library(reticulate)
py_config()
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
library(reticulate)
py_config()
import os
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
library(reticulate)
py_config()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
library(reticulate)
py_config()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
library(reticulate)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(webshot)
library(GGally)
library(recommenderlab)
library(caret)
library(reticulate)
py$c
bf.df = read_csv("BlackFriday.csv") #Se cargan directamente los datos en una tibble
bf.df %>% summary() #Se comprueban los datos que contiene para saber como tratarlos
bf.df %>% str()
bf.df %>% head()
bf.df %>% tail()
bf.df = mutate(bf.df[,-c(10:11)]) %>% na.omit %>% droplevels() #Se eliminan NAs (Especialmente concentrados en las columnas Product_Category 2 y 3)
bf.df = bf.df[!duplicated(bf.df),] #Se eliminan duplicados en caso de haberlos
bf.df$User_ID = bf.df$User_ID - 1000000 #Se convierten User_ID en números del 1 al 6040
bf.df$User_ID = as.factor(bf.df$User_ID) #Se transforman a factor los datos que se considera oportuno
bf.df$Product_ID = as.factor(bf.df$Product_ID)
bf.df$Gender = as.factor(bf.df$Gender)
bf.df$Age = as.factor(bf.df$Age)
bf.df$City_Category = as.factor(bf.df$City_Category)
bf.df$Marital_Status = as.factor(bf.df$Marital_Status)
bf.df$Stay_In_Current_City_Years = as.factor(bf.df$Stay_In_Current_City_Years)
bf.df$Product_Category_1 = as.factor(bf.df$Product_Category_1)
bf.df$Occupation = as.factor(bf.df$Occupation)
bf.df %>% summary() #De nuevo se comprueban los datos para confirmar que ya estén "limpios"
bf.df %>% str()
bf.df = read_csv("BlackFriday.csv") #Se cargan directamente los datos en una tibble
bf.df %>% summary() #Se comprueban los datos que contiene para saber como tratarlos
bf.df %>% str()
bf.df %>% head()
bf.df %>% tail()
bf.df = mutate(bf.df[,-c(10:11)]) %>% na.omit %>% droplevels() #Se eliminan NAs (Especialmente concentrados en las columnas Product_Category 2 y 3)
bf.df = bf.df[!duplicated(bf.df),] #Se eliminan duplicados en caso de haberlos
bf.df$User_ID = bf.df$User_ID - 1000000 #Se convierten User_ID en números del 1 al 6040
bf.df$User_ID = as.factor(bf.df$User_ID) #Se transforman a factor los datos que se considera oportuno
bf.df$Product_ID = as.factor(bf.df$Product_ID)
bf.df$Gender = as.factor(bf.df$Gender)
bf.df$Age = as.factor(bf.df$Age)
bf.df$City_Category = as.factor(bf.df$City_Category)
bf.df$Marital_Status = as.factor(bf.df$Marital_Status)
bf.df$Stay_In_Current_City_Years = as.factor(bf.df$Stay_In_Current_City_Years)
bf.df$Product_Category_1 = as.factor(bf.df$Product_Category_1)
bf.df$Occupation = as.factor(bf.df$Occupation)
bf.df %>% summary() #De nuevo se comprueban los datos para confirmar que ya estén "limpios"
bf.df %>% str()
#Se transforma el data frame pasando cada factor de la columna Product_ID a una variable propia con el ID del producto y de esta manera transformar el data frame en una matriz binaria con 0 en caso de no haber comprado el producto y 1 en caso de si haberlo comprado.
bfcant = bf.df %>%
gather(observation, Val, Product_ID) %>%
group_by(User_ID,observation, Val) %>%
summarise(n= n()) %>%
ungroup() %>%
spread(Val, n, fill=0)
#Se eliminan las columnas 1 y 2 que corresponden a User_ID (ya recogido en el índice) y a una columna que recoge "Product_ID" ya que es el nombre de la columna original.
#De la misma manera únicamente seleccionamos los usuarios con más de 40 compras y los productos adquiridos más de 80 veces, eliminando así todos aquellos usuarios y productos no interesantes para realizar recomendaciones.
bfcant = bfcant[rowSums(bfcant) > 40,
-c(1,2) & colSums(bfcant) > 80]
#Se eliminan las columnas 1 y 2 que corresponden a User_ID (ya recogido en el índice) y a una columna que recoge "Product_ID" ya que es el nombre de la columna original.
#De la misma manera únicamente seleccionamos los usuarios con más de 40 compras y los productos adquiridos más de 80 veces, eliminando así todos aquellos usuarios y productos no interesantes para realizar recomendaciones.
bfcant = bfcant[rowSums(bfcant) > 40,
-c(1,2) & colSums(bfcant) > 80]
#Se eliminan las columnas 1 y 2 que corresponden a User_ID (ya recogido en el índice) y a una columna que recoge "Product_ID" ya que es el nombre de la columna original.
#De la misma manera únicamente seleccionamos los usuarios con más de 40 compras y los productos adquiridos más de 80 veces, eliminando así todos aquellos usuarios y productos no interesantes para realizar recomendaciones.
bfcant = bfcant[rowSums(bfcant) > 40,
-c(1,2) $ colSums(bfcant[,-c(1,2)]) > 80]
#Se eliminan las columnas 1 y 2 que corresponden a User_ID (ya recogido en el índice) y a una columna que recoge "Product_ID" ya que es el nombre de la columna original.
#De la misma manera únicamente seleccionamos los usuarios con más de 40 compras y los productos adquiridos más de 80 veces, eliminando así todos aquellos usuarios y productos no interesantes para realizar recomendaciones.
bfcant = bfcant[rowSums(bfcant) > 40,
colSums(bfcant) > 80]&
bfcant[,-c(1,2)]
#Se eliminan las columnas 1 y 2 que corresponden a User_ID (ya recogido en el índice) y a una columna que recoge "Product_ID" ya que es el nombre de la columna original.
#De la misma manera únicamente seleccionamos los usuarios con más de 40 compras y los productos adquiridos más de 80 veces, eliminando así todos aquellos usuarios y productos no interesantes para realizar recomendaciones.
bfcant = bfcant[,-c(1,2)] %>% bfcant[rowSums(bfcant) > 40,
colSums(bfcant) > 80] &
#Por ultimo se transforma el data frame en una binaryRatingMatrix para poder trabajar y entrenar el modelo.
bf.bin = data.matrix(bfcant)
#Se eliminan las columnas 1 y 2 que corresponden a User_ID (ya recogido en el índice) y a una columna que recoge "Product_ID" ya que es el nombre de la columna original.
#De la misma manera únicamente seleccionamos los usuarios con más de 40 compras y los productos adquiridos más de 80 veces, eliminando así todos aquellos usuarios y productos no interesantes para realizar recomendaciones.
bfcant = bfcant[,-c(1,2)] %>% bfcant[rowSums(bfcant) > 40,
colSums(bfcant) > 80]
#Se eliminan las columnas 1 y 2 que corresponden a User_ID (ya recogido en el índice) y a una columna que recoge "Product_ID" ya que es el nombre de la columna original.
#De la misma manera únicamente seleccionamos los usuarios con más de 40 compras y los productos adquiridos más de 80 veces, eliminando así todos aquellos usuarios y productos no interesantes para realizar recomendaciones.
bfcant = bfcant[,-c(1,2)]
bfcant = bfcant[rowSums(bfcant) > 40,
colSums(bfcant) > 80]
#Por ultimo se transforma el data frame en una binaryRatingMatrix para poder trabajar y entrenar el modelo.
bf.bin = data.matrix(bfcant)
bf.bin = as(bf.bin, "binaryRatingMatrix")
View(bfcant)
bfcant = sparseMatrix(bfcant)
?sparseMatrix
bfcant = as(bfcant, "sparseMatrix")
#Por ultimo se transforma el data frame en una binaryRatingMatrix para poder trabajar y entrenar el modelo.
bf.bin = data.matrix(bfcant)
bfcant = as(bf.bin, "sparseMatrix")
#Se transforma el data frame pasando cada factor de la columna Product_ID a una variable propia con el ID del producto y de esta manera transformar el data frame en una matriz binaria con 0 en caso de no haber comprado el producto y 1 en caso de si haberlo comprado.
bfcant = bf.df %>%
gather(observation, Val, Product_ID) %>%
group_by(User_ID,observation, Val) %>%
summarise(n= n()) %>%
ungroup() %>%
spread(Val, n, fill=0)
#Se eliminan las columnas 1 y 2 que corresponden a User_ID (ya recogido en el índice) y a una columna que recoge "Product_ID" ya que es el nombre de la columna original.
#De la misma manera únicamente seleccionamos los usuarios con más de 40 compras y los productos adquiridos más de 80 veces, eliminando así todos aquellos usuarios y productos no interesantes para realizar recomendaciones.
bfcant = bfcant[,-c(1,2)]
bfcant = bfcant[rowSums(bfcant) > 40,
colSums(bfcant) > 80]
#Por ultimo se transforma el data frame en una binaryRatingMatrix para poder trabajar y entrenar el modelo.
bf.bin = data.matrix(bfcant)
bf.bin = as(bf.bin, "binaryRatingMatrix")
plot(py$clus)
plot(py$clus, main = "Cluster dendrogram")
plot(hclust(py$clus))
